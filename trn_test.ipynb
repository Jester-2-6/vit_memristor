{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "# from models.efficientMem import ViT as ViTMem\n",
    "from models.efficient import ViT as ViT\n",
    "from linformer import Linformer\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.vision_transformer import ViT_B_16_Weights\n",
    "from models.vision_transformer_mem import vit_b_16_mem\n",
    "from torchvision.models import vit_b_16\n",
    "\n",
    "from util.dataset_tools import get_cifar10, train, test, load_model, augment_set\n",
    "from util.adv_tools import loader_to_data, data_to_loader, get_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "should_train = True\n",
    "model_path = 'weights/efficientMem.pth'\n",
    "SEQ_LENGTH = 5\n",
    "LAMBDA = 0.1\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "hidden_dim = 768\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get_cifar10(\n",
    "    batch_size=BATCH_SIZE, new_transforms=[transforms.Resize(224)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficient_transformer = Linformer(\n",
    "#     dim=128,\n",
    "#     seq_len=49 + 1,  # 7x7 patches + 1 cls-token\n",
    "#     depth=12,\n",
    "#     heads=8,\n",
    "#     k=64,\n",
    "# )\n",
    "\n",
    "model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "model_mem = vit_b_16_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "heads_layers = OrderedDict()\n",
    "heads_layers[\"head\"] = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "model.heads = nn.Sequential(heads_layers)\n",
    "model_mem.heads = copy.deepcopy(model.heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3 GPUs\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py\", line 185, in forward\n    outputs = self.parallel_apply(replicas, inputs, module_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py\", line 200, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 110, in parallel_apply\n    output.reraise()\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/_utils.py\", line 694, in reraise\n    raise exception\nRuntimeError: Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torchvision/models/vision_transformer.py\", line 291, in forward\n    x = self._process_input(x)\n        ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torchvision/models/vision_transformer.py\", line 277, in _process_input\n    x = self.conv_proj(x)\n        ^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 460, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument weight in method wrapper_CUDA__cudnn_convolution)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_train:\n\u001b[0;32m----> 2\u001b[0m     train(model, train_loader, test_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, path\u001b[38;5;241m=\u001b[39mmodel_path)\n",
      "File \u001b[0;32m~/chinthana/vit_memristor/util/dataset_tools.py:126\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, epochs, parellel, path)\u001b[0m\n\u001b[1;32m    121\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m#     output, _ = model(data)\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# except ValueError:\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# torch.set_printoptions(profile=\"full\")\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# if batch_idx == 0:\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m#     print(output)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# print(output, target)\u001b[39;00m\n\u001b[1;32m    137\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n",
      "File \u001b[0;32m~/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(replicas)])\n",
      "File \u001b[0;32m~/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:110\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    108\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 110\u001b[0m         output\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m    111\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py\", line 185, in forward\n    outputs = self.parallel_apply(replicas, inputs, module_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py\", line 200, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 110, in parallel_apply\n    output.reraise()\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/_utils.py\", line 694, in reraise\n    raise exception\nRuntimeError: Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torchvision/models/vision_transformer.py\", line 291, in forward\n    x = self._process_input(x)\n        ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torchvision/models/vision_transformer.py\", line 277, in _process_input\n    x = self.conv_proj(x)\n        ^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 460, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/local2/anaconda3/envs/pygpu/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument weight in method wrapper_CUDA__cudnn_convolution)\n\n"
     ]
    }
   ],
   "source": [
    "if should_train:\n",
    "    train(model, train_loader, test_loader, epochs=200, path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model, model_path)\n",
    "model_mem = load_model(model_mem, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [3:50:59<00:00, 175.44s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc Mem: 53.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc_mem, _ = test(model_mem, test_loader, parellel=True)\n",
    "print(f'Acc Mem: {acc_mem}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:06<00:00, 11.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 56.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc, _ = test(model, test_loader, parellel=True)\n",
    "print(f'Acc: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = loader_to_data(test_loader)\n",
    "x_aug, y_aug = augment_set(x_test, y_test, SEQ_LENGTH)\n",
    "test_loader_aug = data_to_loader(x_aug, y_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:45<00:00, 17.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on FGSM (SW): 35.566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [41:55:25<00:00, 193.00s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on FGSM (Memristor): 42.942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not should_train:\n",
    "    x_fgsm = get_attack(\n",
    "        x_test, \"FGSM\", LAMBDA / 10, model=model, input_shape=(3, 224, 224)\n",
    "    )\n",
    "    x_fgsm_aug, y_fgsm_aug = augment_set(x_fgsm, y_test, SEQ_LENGTH)\n",
    "    adv_loader_fgsm = data_to_loader(x_fgsm_aug, y_fgsm_aug)\n",
    "\n",
    "    with open(\"data/c10/adv_loader_fgsm.pkl\", \"wb\") as f:\n",
    "        pickle.dump(adv_loader_fgsm, f)\n",
    "else:\n",
    "    with open(\"data/c10/adv_loader_fgsm.pkl\", \"rb\") as f:\n",
    "        adv_loader_fgsm = pickle.load(f)\n",
    "\n",
    "acc, _ = test(model, adv_loader_fgsm, \"c10_fgsm_sw\")\n",
    "print(\"Accuracy on FGSM (SW):\", acc)\n",
    "\n",
    "acc, _ = test(model_mem, adv_loader_fgsm, \"c10_fgsm_mem\")\n",
    "print(\"Accuracy on FGSM (Memristor):\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
